logging:
    pattern:
        console: "%date{HH:mm:ss.SSS} - %clr(%5level) - [%30.-30thread] - %clr(%-40.-40logger{0}){magenta} - %message%n"
    level:
        org.springframework.ai.vectorstore.SimpleVectorStore: WARN
        org.springframework.ai.mcp: TRACE

info:
    app:
        description: ${project_description}
        name: ${project_artifactId}
        version: ${project_version}

server:
    port: 8082
    servlet:
        context-path: /
    shutdown: graceful

spring:
    config:
        import: optional:file:.env[.properties]
    ai:
        mcp:
            client:
                request-timeout: 60s
                #stdio:
                #    servers-configuration: classpath:mcp-servers.json
                sse:
                    connections:
                        my-mcp-server:
                            url: http://localhost:8081
                toolcallback:
                    enabled: true
                # SYNC or ASYNC if you want to use reactive programming for tool calls.
                type: SYNC
        #openai:
        #   api-key: \${OPENAI_API_KEY}
        #   base-url: ...
        ollama:
            base-url: http://localhost:11434
            chat:
                options:
                    # model: mixtral:8x7B
                    # model: mistral-nemo:12b
                    model: llama3.2

                    # Temperature controls the randomness or "creativity" of the model's response.
                    temperature: 0.5

                    # Limits token selection to the K most likely next tokens.
                    # Higher values (e.g., 40-50) introduce more diversity.
                    topK: 40

                    # (nucleus sampling): Dynamically selects from the smallest set of tokens whose cumulative probability exceeds P.
                    # Values like 0.8-0.95 are common.
                    topP: 0.8

                    # The maxTokens parameter limits how many tokens (word pieces) the model can generate in its response.
                    maxTokens: 1024
            embedding:
                options:
                    model: snowflake-arctic-embed2
                    num-ctx: 8192
            image:
                options:
                    model: llama3.2
                    size: 1024x1024
                    style: vivid
                    quality: standard
                    response-format: url
    application:
        name: mcp-client
    main:
        banner-mode: off
    output:
        ansi:
            enabled: always
    profiles:
        active: memory

# POST http://localhost:8080/ai/actuator/shutdown
management:
    endpoints:
        access:
            default: NONE
        web:
            exposure:
                include: info,health,shutdown
    endpoint:
        health:
            access: READ_ONLY
            show-details: ALWAYS
        info:
            access: READ_ONLY
        shutdown:
            access: unrestricted
    info:
        defaults:
            enabled: true
        env:
            enabled: true
    health:
        defaults:
            enabled: true
