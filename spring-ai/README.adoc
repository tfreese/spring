= Ollama

* Install OLLAMA
** https://github.com/ollama/ollama/blob/main/docs/linux.md[install.sh,window="_blank"] (tar -xvzf ollama-linux-amd64.tgz;)
** or https://github.com/ollama/ollama/blob/main/docs/windows.md[OllamaSetup.exe,window="_blank"]
*** Changing Install Location
- OllamaSetup.exe /DIR="d:/CUSTOM/PATH"

* Path for OLLAMA Models
** Linux
- export OLLAMA_MODELS=/CUSTOM/PATH
** Windows
- set OLLAMA_MODELS=/CUSTOM/PATH

* Pull OLLAMA Models
** open console
- ollama serve
** open console
- ollama pull mixtral:8x7B;             [26.0GB]
- ollama pull mistral-nemo:12b;         [7.1GB]
- ollama pull llama3.2;                 [2.0GB]
- ollama pull snowflake-arctic-embed2;  [1.2GB] // EmbeddingModel for Documents.

* Commands
** Start Ollama
- ollama serve
** Delete Model
- ollama rm mixtral:8x7B
** Run a model
- ollama run mistral-nemo:12b
** List which models are currently loaded
- ollama ps
** Show model information
- ollama show mistral-nemo:12b
** Stop a model which is currently running
- ollama stop mistral-nemo:12b

curl http://localhost:11434/api/embed -d '{
"model": "snowflake-arctic-embed2", "prompt": "The sky is blue because of Rayleigh scattering" }'

https://spring.io/blog/2025/04/14/spring-ai-prompt-engineering-patterns[spring-ai-prompt-engineering-pattern,window="_blank"]

* Configuration

** Temperature

Temperature controls the randomness or "creativity" of the model's response.

- Lower values (0.0-0.3): More deterministic, focused responses.
Better for factual questions, classification, or tasks where consistency is critical.
- Medium values (0.4-0.7): Balanced between determinism and creativity.
Good for general use cases.
- Higher values (0.8-1.0): More creative, varied, and potentially surprising responses.
Better for creative writing, brainstorming, or generating diverse options.

** Output Length (MaxTokens)

The maxTokens parameter limits how many tokens (word pieces) the model can generate in its response.

- Low values (5-25): For single words, short phrases, or classification labels.
- Medium values (50-500): For paragraphs or short explanations.
- High values (1000+): For long-form content, stories, or complex explanations.

** Sampling Controls (Top-K and Top-P)

These parameters give you fine-grained control over the token selection process during generation.

- Top-K: Limits token selection to the K most likely next tokens.
Higher values (e.g., 40-50) introduce more diversity.
- Top-P (nucleus sampling): Dynamically selects from the smallest set of tokens whose cumulative probability exceeds P. Values like 0.8-0.95 are common.

